---
title: "Semantic Specialization in Moe Appears with Scale: A Study of DeepSeek-R1 Expert Specialization"
authors: "Matthew Lyle Olson*, Neale Ratzlaff*, <b>Musashi Hinck</b>*, Man Luo, Sungduk Yu, Chendi Xue, Vasudev Lal"
collection: publications
category: preprints
permalink: /preprints/semanticmoe
excerpt: ''
paperurl: 'https://arxiv.org/abs/2502.10928'
---

## Abstract

DeepSeek-R1, the largest open-source Mixture-of-Experts (MoE) model, has demonstrated reasoning capabilities comparable to proprietary frontier models. Prior research has explored expert routing in MoE models, but findings suggest that expert selection is often token-dependent rather than semantically driven.
Given DeepSeek-R1’s enhanced reasoning abilities, we investigate whether its routing mechanism exhibits greater semantic specialization than previous MoE models. To explore this, we conduct two key experiments: (1) a word sense disambiguation task, where we examine expert activation patterns for words with differing senses, and (2) a cognitive reasoning analysis, where we assess DeepSeek-R1’s structured thought process in an interactive task setting of DiscoveryWorld. We conclude that DeepSeek-R1’s routing mechanism is more semantically aware and it engages in structured cognitive processes. 

